# FAQ's: PromptOps Lifecycle Governance

This artifact is designed for governance-grade prompt versioning, evaluation, and rollback. But like any dev-facing system, it must stand up to engineering scrutiny. This doc surfaces common objections engineers may have.

## âŒ â€œWhy do we need prompt versioning? Git already versions everything.â€

Git doesnâ€™t capture:

- Prompt approval state
- Eval pass/fail status
- Cost risk
- Production vs staging context

This artifact tracks prompt _behavior_ â€” not just file diffs. Think: semantic versioning, human approval, eval compliance.

## âŒ â€œPromptfoo isn't the industry standard â€” why use it?â€

Promptfoo is a reference implementation. The artifact is model-agnostic â€” swap in LangSmith, Guardrails, or Claude CLI outputs. Promptfoo helps prove CI integration and local testability.

## âŒ â€œToo much ceremony. Just want to test prompts fast.â€

Use the `minimal/` setup:

- One schema
- One eval test
- One log

Governs the most critical risk: silent regression.

## âŒ â€œThis doesnâ€™t support agents or RAG.â€

True â€” this artifact governs **individual prompts**. Itâ€™s composable across:

- Agent nodes (LangGraph)
- Retrieval chains (LlamaIndex, Pinecone)

Agent and RAG-specific governance lives in future artifacts (e.g. Debugging Copilot, Agent Design Matrix).

## âŒ â€œHow do I integrate this into my existing repo?â€

Use `docs/integration-guide.md` (coming soon):

- LangChain: version + test your prompt node
- Cursor: use log schema + HITL tags for prompt panels
- Claude: track eval pass/fail across versions

## âŒ â€œThis is too GPT/Claude specific.â€

Prompt evaluation is model-agnostic. We've tested with:

- GPT-4 / Claude / Cohere / Ollama
- Any API-returning model is supported
- Add your provider config in `evalsuite-core-config.yml`

## âŒ â€œNo enforcement â€” devs can ignore the schema.â€

CI gates (`ci/eval-gate.yml`) enforce:

- Promptfoo eval thresholds
- Schema validity
- Prompt log presence
- Approval field checks

Hard-mode branches include pre-merge guards and PR blockers.

## âŒ â€œRollback sounds manual.â€

Yes â€” rollback is traceable, not magic. It includes:

- Restoring eval-passing prompt
- Updating schema version
- Triggering new eval run
- Logging the rollback decision

## âŒ â€œWe move too fast for this kind of governance.â€

Use governance modes:

- ğŸŸ¢ **Fast team:** 1 schema + 1 eval
- ğŸŸ¡ **Growing org:** CI eval + version tags
- ğŸ”´ **Governed org:** Approval + rollback + audit trail

## âŒ â€œWe use LangSmith already â€” isnâ€™t that enough?â€

LangSmith is great for tracing â€” this artifact adds **version enforcement**, **CI gating**, and **structured rollback triggers**.

Use LangSmith for UI traceability, PromptOps for lifecycle control.

## âŒ â€œWhere do we store the logs?â€

Logs are schema-bound JSON. Store locally, in S3, or pipe to OpenTelemetry-compatible systems.

Schema: [`schemas/prompt-log-schema.json`](../schemas/prompt-log-schema.json)

## âŒ â€œToo much config. Can't we just hardcode prompts?â€

You can â€” but you lose:

- Drift detection
- Cost awareness
- Approval history
- CI enforcement

This artifact separates prompt _intent_ (the YAML) from _execution_ (the LLM call).

## âŒ â€œWhat if my team just ignores this?â€

PromptOps is opt-in â€” but CI hooks, approval tags, and eval gates make non-compliance visible.

Start with the `minimal/` path and build trust through automation, not mandates.

---

## Want to Help?

Clone the starter repo, remix an eval config, or file a GitHub issue with a real objection. Weâ€™ll test it, log it, and ship a fix.
