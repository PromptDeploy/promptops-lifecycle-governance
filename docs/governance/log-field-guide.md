# Prompt Logging Guide: Structure Logs for Audit, Debugging, and Cost Analysis

This document explains the meaning, format, and governance role of each field in the `prompt-log-schema.json` file.

Used by:

- Teams integrating trace logging
- Auditors reviewing prompt behavior
- Engineers debugging drift, cost spikes, or eval regressions

---

## Required Fields

| Field             | Type                 | Description                                                                                               |
| ----------------- | -------------------- | --------------------------------------------------------------------------------------------------------- |
| **prompt_id**     | `string`             | Unique ID or name of the prompt run (e.g., `"rag-qa-prompt-v1"`). Should map to versioned prompt schemas. |
| **version_hash**  | `string`             | SHA256 (or equivalent) hash of the full prompt content. Ensures content immutability and drift detection. |
| **timestamp**     | `string` (`ISO8601`) | When the prompt was executed. Use UTC to align across logs.                                               |
| **trace_id**      | `string`             | Shared across related runs (multi-agent flows, retries). Enables span-like tracing.                       |
| **model**         | `string`             | The LLM used (e.g., `"gpt-4"`, `"claude-3"`). Useful for debugging and cost tracking.                     |
| **tokens_input**  | `integer`            | Number of input tokens sent to the model.                                                                 |
| **tokens_output** | `integer`            | Number of output tokens generated by the model.                                                           |
| **cost_usd**      | `number`             | Estimated cost in USD, based on token pricing at runtime.                                                 |
| **inputs**        | `object`             | Structured input fields passed into the prompt. Can vary by task.                                         |
| **outputs**       | `object`             | Raw model outputs, including structured responses or metadata.                                            |

---

## Optional Fields

| Field              | Type            | Description                                                                                                             |
| ------------------ | --------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **retrieved_docs** | `array[string]` | For RAG pipelines, list of source documents retrieved and injected into the prompt. Useful for hallucination debugging. |
| **eval_results**   | `object`        | Results of post-run evaluation. Can include `pass/fail`, numeric score, criteria tags, or metric breakdowns.            |
| **duration_ms**    | `integer`       | Model response latency in milliseconds. Helps detect slow completions.                                                  |
| **run_id**         | `string`        | Optional opaque ID for linking this log to a frontend tool or monitoring dashboard. Can be generated by caller.         |

---

## Governance Notes

- **version_hash** is your prompt fingerprint â€” treat it as canonical for reproducibility.
- **trace_id** allows multi-agent or multi-hop workflows to be fully traced.
- **cost_usd** + `tokens_*` enable pricing guardrails and regression gates.
- **eval_results** should be machine-generated (e.g. via Promptfoo) to support CI/CD enforcement.

---

> For a live example of a fully populated trace, see [`log-sample.output.json`](./log-sample.output.json)
> For evaluation config mapping, see [`evals/eval-config.promptfoo.yml`](../evals/eval-config.promptfoo.yml)
